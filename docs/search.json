[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi There! I am Stephen Lemasney. I work in FinTech as a Sales Engineer. In this blog I share some Python and data tutorials."
  },
  {
    "objectID": "build-a-test-csv-dataset.html",
    "href": "build-a-test-csv-dataset.html",
    "title": "Blog",
    "section": "",
    "text": "In this tutorial, we’ll build a test data set. The dataset will be a simple real estate dataset showing attributes such as price, number of bedrooms, parking etc. The data is entirely fabricated. Once created, we’ll also show how you can load the data set as a CSV onto AWS S3.\n\n\n\nimport pandas as pd\nimport tomli\nimport pprint\n\n# We'll use random type functions to generate the random house price data\nimport random\n\n\n\n\nWe’ll store our config in a TOML file. The config will contain basic instructions for creating the test data. There are 4 tiers, each with different constraints. For example, in tier 1, we create house with prices in the range 100k to 200k.\nContents of config/housing.toml\n# Config for creating a housing dataset\n\n[tier-1]\nprice = \"100000-200000\"\nbedrooms = \"1-3\"\nbathrooms = \"1-3\"\nestate_agent_code = \"1-5\" \ntransport_link_code = \"1-8\"\ncouncil_tax = [\"A\", \"B\", \"C\"]\nfreehold = [true, false]\ngarage = [false]\nparking = [true, false]\n\n[tier-2]\n...\n\nwith open('config/housing.toml', 'rb') as file_obj:\n    housing_config = tomli.load(file_obj)\n    \npprint.pprint(housing_config[\"tier-1\"])\n\n{'bathrooms': '1-3',\n 'bedrooms': '1-3',\n 'council_tax': ['A', 'B', 'C'],\n 'estate_agent_code': '1-5',\n 'freehold': [True, False],\n 'garage': [False],\n 'parking': [True, False],\n 'price': '100000-200000',\n 'transport_link_code': '1-8'}\n\n\n\n\n\nNext we build our random DataFrame using the TOML file. We use two main functions (plus some helper functions):\n\nThe first function generate_one_row_random_house_data creates a random row of data per the config specification\nThe second main function generate_housing_df uses the first function to create a large sample of data\n\nAs always we try to keep our functions small:\n\n“The first rule of functions is that they should be small. The second rule of functions is that they should be smaller than that.” -Robert C. Martin, Clean Code: A Handbook of Agile Software Craftsmanship\n\n\ndef get_randint_from_range_str(range_str):\n    \n    \"\"\"\n    Randomly returns an int in range\n    The range is parsed from string in the form '2-10'\n    If the value is greater than 100000, round to 3 decimal places \n    \"\"\"\n    \n    range_list = [int(i) for i in range_str.split(\"-\")]\n    \n    min_value, max_value = min(range_list), max(range_list)\n    \n    rand_int = random.randint(min_value, max_value)\n    \n    # For bigger numbers, such as price, we round to closest thousand\n    # House prices are more likely to be quoted rounded to the nearest thousand\n    \n    return rand_int if rand_int <= 100000 else round(rand_int, -3)\n\n\ndef get_random_tier_config(housing_config):\n    \n        \"\"\"\n        Returns tier config for one tier.\n        The config set is selected randomly\n        \"\"\"\n    \n        tiers = list(housing_config.keys())\n\n        random_tier = random.choice(tiers)\n\n        tier_config = housing_config[random_tier]\n    \n        return tier_config\n\n\ndef generate_one_row_random_house_data(config):\n    \n    \"\"\"\n    Returns one row of data in dict format\n    The data is randomly produced using constraints in the config file\n    \"\"\"\n\n    new_row = {}\n\n    for housing_attribute in config:\n        \n        housing_attribute_value = config[housing_attribute]\n        \n        if type(housing_attribute_value) == str:\n            \n            # Note:\n            # In production, we'd employ some defensive programming here to ensure that strings\n            # loaded from the config are always in the form \"{int}-{int}\". For example: 2-8\n                        \n            new_row[housing_attribute] = get_randint_from_range_str(housing_attribute_value)\n\n        if type(housing_attribute_value) == list:\n\n            new_row[housing_attribute] = random.choice(housing_attribute_value)\n            \n    return new_row\n\n\ndef generate_housing_df(housing_config, row_size=1000000):\n    \n    \"\"\"\n    Returns a dataframe of random housing data per \n    constraints outlined in TOML file.\n    The number of rows is specified in the parameters.\n    \"\"\"\n\n    df_rows = []\n\n    for i in range(row_size):\n\n        tier_config = get_random_tier_config(housing_config)\n\n        new_row = generate_one_row_random_house_data(tier_config)\n\n        df_rows.append(new_row)\n\n    return pd.DataFrame(df_rows)\n\n\ndf = generate_housing_df(housing_config)\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      price\n      bedrooms\n      bathrooms\n      estate_agent_code\n      transport_link_code\n      council_tax\n      freehold\n      garage\n      parking\n    \n  \n  \n    \n      0\n      304000\n      4\n      3\n      4\n      7\n      F\n      True\n      True\n      False\n    \n    \n      1\n      218000\n      2\n      3\n      3\n      7\n      B\n      False\n      True\n      False\n    \n    \n      2\n      100000\n      1\n      2\n      5\n      8\n      C\n      True\n      False\n      True\n    \n  \n\n\n\n\n\n# Check the statistical dispersion of the sample data\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      price\n      bedrooms\n      bathrooms\n      estate_agent_code\n      transport_link_code\n    \n  \n  \n    \n      count\n      1000000.000000\n      1000000.000000\n      1000000.000000\n      1000000.000000\n      1000000.000000\n    \n    \n      mean\n      362416.987000\n      3.623684\n      3.623902\n      3.623851\n      5.124652\n    \n    \n      std\n      225967.881825\n      1.705249\n      1.703965\n      1.148088\n      1.985941\n    \n    \n      min\n      100000.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n    \n    \n      25%\n      200000.000000\n      2.000000\n      2.000000\n      3.000000\n      4.000000\n    \n    \n      50%\n      300000.000000\n      3.000000\n      3.000000\n      4.000000\n      5.000000\n    \n    \n      75%\n      400000.000000\n      4.000000\n      4.000000\n      5.000000\n      7.000000\n    \n    \n      max\n      1000000.000000\n      8.000000\n      8.000000\n      5.000000\n      8.000000\n    \n  \n\n\n\n\n\n\n\n\ndf.to_csv(\"data/housing-data.csv\", index=False)\n\n\n\n\nFinally, we save the CSV file to S3. Fortunately, the Pandas API does all of the “heavy lifting” to connect to AWS.\n\n%%script echo skipping\n\nS3_BUCKET_LOCATION = \"<BUCKET_LOCATION>\"\n\ndf.to_csv(S3_BUCKET_LOCATION)\n\nskipping"
  },
  {
    "objectID": "create-database-using-sqlite.html",
    "href": "create-database-using-sqlite.html",
    "title": "Blog",
    "section": "",
    "text": "In this tutorial, we’ll create a simple house price data database using SQLite.\n\n\nFirst off, we use Pandas to take a look at the data we want to load into SQLite. We add a “market_id” column which is the unique identifier per row. Later we’ll use this a primary key in a table.\n\nimport pandas as pd\n\n\ndf = pd.read_csv(\"data/housing-data.csv\")[:20]\n\n## Add a market_id column which will become the primary key for the table below\n\ndf[\"market_id\"] = [ f\"prop_{i}\" for i in range(len(df))]\n\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      price\n      bedrooms\n      bathrooms\n      estate_agent_code\n      transport_link_code\n      council_tax\n      freehold\n      garage\n      parking\n      market_id\n    \n  \n  \n    \n      0\n      304000\n      4\n      3\n      4\n      7\n      F\n      True\n      True\n      False\n      prop_0\n    \n    \n      1\n      218000\n      2\n      3\n      3\n      7\n      B\n      False\n      True\n      False\n      prop_1\n    \n  \n\n\n\n\n\n\n\nNext we create an empty database called “housing.db” which we store in the local file system.\n\nfrom sqlalchemy import create_engine\nfrom pathlib import Path\n\n\ndatabase_relative_path = \"data/housing.db\"\n\ndatabase_path = Path.cwd().joinpath(database_relative_path)\n\nengine = create_engine(f'sqlite:///{database_path}', echo=True)\n\n\n\n\nThen we create some models using sqlalchemy to map Python objects into the database schema. For a more in depth example, have a look at this page.\n\nfrom typing import List\nfrom typing import Optional\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy import String, Float, Integer\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\nfrom sqlalchemy.orm import relationship\n\nclass Base(DeclarativeBase):\n    pass\n\nclass HousingMarket(Base):\n    \n    __tablename__ = \"housing_market\"\n    \n    market_id: Mapped[str] = mapped_column(primary_key=True)\n    price: Mapped[float] = mapped_column(Float())\n    bedrooms: Mapped[int] = mapped_column(Integer())\n    bathrooms: Mapped[int] = mapped_column(Integer())\n    estate_agent_code: Mapped[int] = mapped_column(Integer())\n    transport_link_code: Mapped[int] = mapped_column(Integer())\n    council_tax: Mapped[str] = mapped_column(String())\n    freehold: Mapped[int] = mapped_column(Integer())\n    garage: Mapped[int] = mapped_column(Integer()) \n    parking: Mapped[int] = mapped_column(Integer()) \n        \nclass TransportCodes(Base):\n    \n    __tablename__ = \"transport_codes\"\n    \n    transport_code: Mapped[int] = mapped_column(ForeignKey(\"housing_market.transport_link_code\"), primary_key=True)\n    transport_string: Mapped[str] = mapped_column(String())\n\n        \nclass EstageAgentCodes(Base):\n    \n    __tablename__ = \"estate_agent_codes\"\n    \n    estate_agent_code: Mapped[int] = mapped_column(ForeignKey(\"housing_market.estate_agent_code\"), primary_key=True)\n    estate_agent_string: Mapped[str] = mapped_column(String())\n        \n\n\nBase.metadata.create_all(engine)\n\n2023-02-26 17:02:44,001 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"housing_market\")\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"housing_market\")\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"transport_codes\")\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine PRAGMA main.table_info(\"estate_agent_codes\")\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine PRAGMA temp.table_info(\"estate_agent_codes\")\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine [raw sql] ()\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine \nCREATE TABLE housing_market (\n    market_id VARCHAR NOT NULL, \n    price FLOAT NOT NULL, \n    bedrooms INTEGER NOT NULL, \n    bathrooms INTEGER NOT NULL, \n    estate_agent_code INTEGER NOT NULL, \n    transport_link_code INTEGER NOT NULL, \n    council_tax VARCHAR NOT NULL, \n    freehold INTEGER NOT NULL, \n    garage INTEGER NOT NULL, \n    parking INTEGER NOT NULL, \n    PRIMARY KEY (market_id)\n)\n\n\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine [no key 0.00049s] ()\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine \nCREATE TABLE estate_agent_codes (\n    estate_agent_code INTEGER NOT NULL, \n    estate_agent_string VARCHAR NOT NULL, \n    PRIMARY KEY (estate_agent_code), \n    FOREIGN KEY(estate_agent_code) REFERENCES housing_market (estate_agent_code)\n)\n\n\n2023-02-26 17:02:44,003 INFO sqlalchemy.engine.Engine [no key 0.00041s] ()\n2023-02-26 17:02:44,021 INFO sqlalchemy.engine.Engine COMMIT\n\n\n\n\n\nThe we populate our tables with data using the data from the DataFrame.\n\nfrom sqlalchemy.orm import Session\n\nwith Session(engine) as session:\n    \n    db_rows = []\n    \n    for index, row in df.iterrows():\n        \n        house_item = HousingMarket(\n        \n            market_id = row.market_id,\n            price = row.price,\n            bedrooms = row.bedrooms,\n            bathrooms = row.bathrooms,\n            estate_agent_code = row.estate_agent_code,\n            transport_link_code = row.transport_link_code,\n            council_tax = row.council_tax,\n            freehold = row.freehold,\n            garage = row.garage,\n            parking = row.parking,\n        )\n        \n        db_rows.append(house_item)\n        \n    session.add_all(db_rows)\n    session.commit()\n\n2023-02-26 17:02:44,038 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-02-26 17:02:44,038 INFO sqlalchemy.engine.Engine INSERT INTO housing_market (market_id, price, bedrooms, bathrooms, estate_agent_code, transport_link_code, council_tax, freehold, garage, parking) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n2023-02-26 17:02:44,038 INFO sqlalchemy.engine.Engine [generated in 0.00069s] [('prop_0', 304000.0, 4, 3, 4, 7, 'F', True, True, False), ('prop_1', 218000.0, 2, 3, 3, 7, 'B', False, True, False), ('prop_2', 100000.0, 1, 2, 5, 8, 'C', True, False, True), ('prop_3', 758000.0, 4, 3, 4, 4, 'D', True, False, True), ('prop_4', 171000.0, 2, 2, 5, 2, 'B', True, False, False), ('prop_5', 764000.0, 5, 5, 5, 7, 'F', True, False, True), ('prop_6', 369000.0, 4, 3, 3, 7, 'D', True, False, True), ('prop_7', 158000.0, 2, 2, 2, 1, 'A', False, False, True)  ... displaying 10 of 20 total bound parameter sets ...  ('prop_18', 382000.0, 3, 5, 4, 6, 'F', True, True, False), ('prop_19', 128000.0, 2, 2, 5, 7, 'A', True, False, True)]\n2023-02-26 17:02:44,038 INFO sqlalchemy.engine.Engine COMMIT\n\n\n\nestate_agent_mapping = {\n    \n    1: \"Clapham\",\n    2: \"Deptford\",\n    3: \"Brixton\",\n    4: \"Richmond\",\n    5: \"Isle of Dogs\"\n}\n\n\nfrom sqlalchemy.orm import Session\n\nwith Session(engine) as session:\n    \n    db_rows = []\n    \n    for code in estate_agent_mapping:\n        \n        estate_agent_code = EstageAgentCodes(\n        \n            estate_agent_code = code,\n            estate_agent_string = estate_agent_mapping.get(code),\n        )\n        \n        db_rows.append(estate_agent_code)\n        \n    session.add_all(db_rows)\n    session.commit()\n\n2023-02-26 17:02:44,075 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-02-26 17:02:44,075 INFO sqlalchemy.engine.Engine INSERT INTO estate_agent_codes (estate_agent_code, estate_agent_string) VALUES (?, ?)\n2023-02-26 17:02:44,075 INFO sqlalchemy.engine.Engine [generated in 0.00043s] [(1, 'Clapham'), (2, 'Deptford'), (3, 'Brixton'), (4, 'Richmond'), (5, 'Isle of Dogs')]\n2023-02-26 17:02:44,075 INFO sqlalchemy.engine.Engine COMMIT\n\n\n\n\n\nFinally, to verify that the data has been loaded correctly, we run a test query.\n\nsql_query_string = \"\"\"\nSELECT \neac.estate_agent_string,\nhm.market_id,\nhm.price, \nhm.bedrooms \nFROM housing_market hm\nJOIN estate_agent_codes eac on (hm.estate_agent_code = eac.estate_agent_code)\nwhere eac.estate_agent_string in (\"Richmond\", \"Clapham\")\n\n\"\"\"\n\n\nfrom sqlalchemy import sql\nfrom  sqlalchemy.exc import OperationalError\nfrom tabulate import tabulate\n\nstr_sql = sql.text(sql_query_string)\n\nwith Session(engine) as session:\n    \n    query = session.execute(str_sql)\n    \n    query_results = query.fetchall()\n        \n    print(tabulate(query_results, headers=query.keys(), tablefmt='sqlite'))\n\n2023-02-26 17:02:44,119 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-02-26 17:02:44,120 INFO sqlalchemy.engine.Engine \nSELECT \neac.estate_agent_string,\nhm.market_id,\nhm.price, \nhm.bedrooms \nFROM housing_market hm\nJOIN estate_agent_codes eac on (hm.estate_agent_code = eac.estate_agent_code)\nwhere eac.estate_agent_string in (\"Richmond\", \"Clapham\")\n\n\n2023-02-26 17:02:44,122 INFO sqlalchemy.engine.Engine [generated in 0.00106s] ()\nestate_agent_string    market_id      price    bedrooms\n---------------------  -----------  -------  ----------\nRichmond               prop_0        304000           4\nRichmond               prop_3        758000           4\nRichmond               prop_8        228000           4\nRichmond               prop_9        209000           2\nRichmond               prop_16       313000           3\nRichmond               prop_18       382000           3\n2023-02-26 17:02:44,122 INFO sqlalchemy.engine.Engine ROLLBACK\n\n\n\n\n\nThen we drop the table, to keep the example idempotent for the next run.\n\ndef drop_tables(table_name):\n\n    str_sql = sql.text(f\"drop table {table_name}\")\n\n    with Session(engine) as session:\n\n        try:\n\n            query = session.execute(str_sql)\n\n        except OperationalError as e:\n\n            print(e)\n            \n\ndrop_tables(\"estate_agent_codes\")\ndrop_tables(\"housing_market\")\n\n2023-02-26 17:02:44,140 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-02-26 17:02:44,140 INFO sqlalchemy.engine.Engine drop table estate_agent_codes\n2023-02-26 17:02:44,140 INFO sqlalchemy.engine.Engine [generated in 0.00083s] ()\n2023-02-26 17:02:44,153 INFO sqlalchemy.engine.Engine ROLLBACK\n2023-02-26 17:02:44,154 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n2023-02-26 17:02:44,156 INFO sqlalchemy.engine.Engine drop table housing_market\n2023-02-26 17:02:44,156 INFO sqlalchemy.engine.Engine [generated in 0.00089s] ()\n2023-02-26 17:02:44,156 INFO sqlalchemy.engine.Engine ROLLBACK"
  },
  {
    "objectID": "flask-app-with-docker.html",
    "href": "flask-app-with-docker.html",
    "title": "Blog",
    "section": "",
    "text": "In this tutorial, we show how you can build and deploy a really simple Python app using Docker. To demonstrate, we’ll create an API to return details of the TFL train lines in London. We’ll use Flask to build the application.\n\n\nFirst, create a python flask app which returns details about the TFL trains. This application serves a very basic API with details on London’s TFL trains.\n\n#collapse-hide\n\nfrom flask import Flask, jsonify, request\nimport sys\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\napp = Flask(__name__)\n\ntasks = [\n    {\n        'id': 1,\n        'service': 'tube',\n        'line': 'northern',\n        'colour': 'black'\n    },\n    {\n        'id': 2,\n        'service': 'tube',\n        'line': 'circle',\n        'colour': 'red'\n    }\n]\n\ndef shutdown_server():\n    func = request.environ.get('werkzeug.server.shutdown')\n    if func is None:\n        raise RuntimeError('Not running with the Werkzeug Server')\n    func()\n\n@app.route('/trains', methods=['GET'])\ndef get_tasks():\n    return jsonify({'tasks': tasks})\n\n@app.route('/')\ndef hello_world():\n    return 'Welcome to trains API'\n\n@app.route('/exit')\ndef exit():\n    message = logging.info(\"Stopping application\")\n    shutdown_server()\n    print(\"The Flask server has been shutdown.\")\n\nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0')\n\n\n\n\n\nimport requests\n\nurl = \"http://192.168.1.74:5000/\"\n\nresponse = requests.get(url)\nresponse_code = response.status_code\nresponse_text = requests.get(url).text\n\ndisplay(response_code)\ndisplay(response_text)\n\n200\n\n\n'Welcome to trains API'\n\n\n\n\n\nNext, we want to package the application into a container using a Dockerfile. The finished Dockerfile will look as follows:\nFROM python:3.7-alpine\n\nCOPY . /app\n\nWORKDIR /app\n\nRUN pip install -r requirements.txt\n\nEXPOSE 5000\n\nCMD [\"python\", \"app.py\"]\n\n\n\nLet’s break down each line in turn to describe what is happening…\nIn the first line we declare a parent image which is the image our own image is based on. Each subsequent declaration in the Dockerfile modifies this image. In our example, we use a version of Alpine Linux as our base image. Alpine Linux is a lightweight Linux distribution which makes it ideal for our container.\nFROM python:3.7-alpine\nNext we copy all of the files from the current host directory into the container’s app directory.\nCOPY . /app\nWe change our working directory to the app directory. And then we tell Docker to install the Python packages needed for the app.py\nWORKDIR /app\n\nRUN pip install -r requirements.txt\nNext we tell the container to listen on a specific network port at runtime. The default is TCP but you can also specify UDP. Note that the EXPOSE instruction does not actually publish the port. This instruction is there for documentation purposes. To actually publish the port, you need to use the -p flag on the docker run command.\nEXPOSE 5000\nFinally, we run the app:\nCMD [\"python\", \"app.py\"]\n\n\n\nYou might want to run the Dockerfile on your own machine to verify that it is working correctly. To do that, we docker build and then docker run:\nYou can build the Dockerfile as follows. The -t or tag allows you to tag the image with a name.\ndocker build -t slemasne/trains .\nThen run the image to create a container which runs our application. The ‘-p’ flag maps port 5000 on localhost in the host to port 5000 in the docker container.\ndocker run -p 5000:5000 slemasne/trains\nYou can also run the command with a ‘-d’ detached flag to run the container in the background:\ndocker run -d -p 5000:5000 slemasne/trains\nThe application can be accessed on our localhost:\nhttp://localhost:5000/trains"
  },
  {
    "objectID": "flask-app-with-k8s.html",
    "href": "flask-app-with-k8s.html",
    "title": "Blog",
    "section": "",
    "text": "In Part 1 of this tutorial, we deployed a simple Flask app using docker. In this part, we’ll deploy that same app using Kubernetes.\n\n\nFor the Kubernetes deployment, we’ll need to create a deployment for managing pods/containers.\nWe create the deployment using a YAML file.\ntrains-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trains\nspec:\n  selector:\n    matchLabels:\n      app: trains\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: trains\n    spec:\n      containers:\n      - name: trains\n        image: slemasne/trains:latest\n        resources:\n          limits:\n            memory: \"100Mi\"\n          requests:\n            memory: \"50Mi\"\n        ports:\n        - containerPort: 5000\n\n\n\nWe can see that two pods are running using the following command:\n$ kubectl get pods\n\nNAME                      READY   STATUS    RESTARTS   AGE\ntrains-758d4f498c-kbs9k   1/1     Running   0          6m19s\ntrains-758d4f498c-kfbpq   1/1     Running   1          6m19s\n\n\n\nNext we run a port-forward command so we can connect to the cluster from our local machine.\nkubectl port-forward deployment/trains 5000:5000\nWe check the pod is online by running a curl command.\n$ curl -I http://127.0.0.1:5000/\nThis command returns a status code of 200:\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 21\nServer: Werkzeug/1.0.1 Python/3.7.9\nDate: Wed, 14 Sep 2022 18:01:56 GMT"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stephen Lemasney",
    "section": "",
    "text": "A blog of mostly Python and data tutorials.\nBuild a test CSV dataset\nCreate an SQLite database with data from a DataFrame\nRun a Postres Database locally\nData analysis on long distance running\nDeploying a Python app - Part 1 (Docker)\nDeploying a Python app - Part 2 (Kubernetes)\nMutable and immutable objects in Python"
  },
  {
    "objectID": "mutability.html",
    "href": "mutability.html",
    "title": "Blog",
    "section": "",
    "text": "In this post, we’ll have a look at mutable versus immutable objects in Python. Mutable objects are objects we can change whereas immutable objects are objects we cannot change.\nBefore we jump in, let’s have a quick look at the id function in Python. From the official Python docs:\nReturn the “identity” of an object. This is an integer which is guaranteed to be unique and constant for this object during its lifetime. Two objects with non-overlapping lifetimes may have the same id() value.\nWhy is this function helpful? We can use this function to verify that a mutable object has the same ID after we have changed it.\n\n# Create helper function to print object memory ID\n\ndef log_memory_address_id(a):\n    \n    print(f\"The list has internal memory address id: {id(a)}\")\n\n\n\nLet’s create a list to represent three of London’s tube lines:\n\na = [\"nothern\", \"central\", \"circle\"]\n\nprint(a)\n\nlog_memory_address_id(a)\n\n['nothern', 'central', 'circle']\nThe list has internal memory address id: 2454400216128\n\n\nNext we append a new line to the list and verify that the list has the same ID:\n\na.append(\"city\")\n\nprint(a)\n\nlog_memory_address_id(a)\n\n['nothern', 'central', 'circle', 'city']\nThe list has internal memory address id: 2454400216128\n\n\n\n\n\nNext we’ll have a look at strings. This time, however, we cannot modify a string object after it has been declared. Instead we need to declare a copy of the string object with any modifications (at the point of string initialization).\n\ntube_line = \"northern\"\n\nid(tube_line)\n\n2454399439024\n\n\n\n# The new object has a different Id\n\nid(tube_line + \"_tfl\")\n\n2454401150000\n\n\n\n\n\nBelow we’ve provided a summary of the mutability of other Python objects:\n\n\n\nObject\nImmutable?\n\n\n\n\nbool\nY\n\n\nint\nY\n\n\nfloat\nY\n\n\ntuple\nY\n\n\nstring\nY\n\n\nlist\nN\n\n\nset\nN\n\n\ndict\nN"
  },
  {
    "objectID": "pandas-cheatsheet.html",
    "href": "pandas-cheatsheet.html",
    "title": "Blog",
    "section": "",
    "text": "In this tutorial, we’ll have a look at the pandas library\n\nFiltering by attributes in the data set\nRunning aggregations (such as mean, count etc)\nApplying a where clause to label the data\nAdd column specific formatting\n\n\nimport pandas as pd\n\nhousing_df = pd.read_csv(\"data/housing-data.csv\")\n\n\n\nWe filter using the approach below.\nTwo things to note here (which often used to catch me out):\n\nYou can only use the & and | operators (not and and or)\nMultiple boolean statements must be separated by parentheses\n\n\nfour_bed_with_garage = housing_df[(housing_df.bedrooms ==4) & (housing_df.garage==True)]\nfour_bed_with_garage.head(3)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      price\n      bedrooms\n      bathrooms\n      council_tax\n      freehold\n      garage\n      parking\n    \n  \n  \n    \n      6\n      6\n      220000\n      4\n      2\n      C\n      False\n      True\n      False\n    \n    \n      11\n      11\n      395000\n      4\n      4\n      F\n      True\n      True\n      False\n    \n    \n      19\n      19\n      334000\n      4\n      4\n      F\n      True\n      True\n      True\n    \n  \n\n\n\n\n\ngreater_than_two_bathrooms = housing_df[housing_df.bedrooms > 2]\ngreater_than_two_bathrooms.head(3)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      price\n      bedrooms\n      bathrooms\n      council_tax\n      freehold\n      garage\n      parking\n    \n  \n  \n    \n      0\n      0\n      972000\n      4\n      7\n      D\n      True\n      False\n      True\n    \n    \n      1\n      1\n      450000\n      4\n      5\n      D\n      True\n      False\n      True\n    \n    \n      2\n      2\n      754000\n      5\n      7\n      D\n      True\n      False\n      True\n    \n  \n\n\n\n\n\n\n\n\naverage_price_per_no_of_bedrooms = housing_df[[\"bedrooms\", \"price\"]].groupby(\"bedrooms\").mean()\naverage_price_per_no_of_bedrooms\n\n\n\n\n\n  \n    \n      \n      price\n    \n    \n      bedrooms\n      \n    \n  \n  \n    \n      1\n      149998.814442\n    \n    \n      2\n      199846.936087\n    \n    \n      3\n      314227.749105\n    \n    \n      4\n      380126.039318\n    \n    \n      5\n      465421.410331\n    \n    \n      6\n      698804.466716\n    \n    \n      7\n      699474.242897\n    \n    \n      8\n      699945.904285\n    \n  \n\n\n\n\n\n\n\nThere are multiple ways of making new columns.\n\n\n\n\nhousing_df[\"size\"] = housing_df[\"bedrooms\"].apply(lambda x: )\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      price\n      bedrooms\n      bathrooms\n      council_tax\n      freehold\n      garage\n      parking\n      size\n    \n  \n  \n    \n      0\n      0\n      972000\n      4\n      7\n      D\n      True\n      False\n      True\n      Medium\n    \n    \n      1\n      1\n      450000\n      4\n      5\n      D\n      True\n      False\n      True\n      Medium\n    \n    \n      2\n      2\n      754000\n      5\n      7\n      D\n      True\n      False\n      True\n      Medium\n    \n    \n      3\n      3\n      134000\n      2\n      1\n      A\n      False\n      False\n      True\n      Medium\n    \n    \n      4\n      4\n      316000\n      5\n      5\n      F\n      True\n      True\n      False\n      Medium\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      999995\n      999995\n      275000\n      2\n      3\n      C\n      False\n      True\n      True\n      Medium\n    \n    \n      999996\n      999996\n      326000\n      4\n      4\n      D\n      True\n      False\n      False\n      Medium\n    \n    \n      999997\n      999997\n      204000\n      2\n      2\n      D\n      True\n      True\n      True\n      Medium\n    \n    \n      999998\n      999998\n      247000\n      4\n      4\n      C\n      False\n      False\n      False\n      Medium\n    \n    \n      999999\n      999999\n      805000\n      6\n      8\n      F\n      True\n      True\n      True\n      Big\n    \n  \n\n1000000 rows × 9 columns"
  },
  {
    "objectID": "polars-cheatsheet.html",
    "href": "polars-cheatsheet.html",
    "title": "Blog",
    "section": "",
    "text": "In this tutorial, we’ll have a look at polars, the new ultra fast library for processing DataFrames. In particular, we’ll look at the following:\n\nFiltering by attributes in the data set\nRunning aggregations (such as mean, count etc)\nApplying a where clause to label the data\nAdd column specific formatting\n\n\nimport polars as pl\n\nhousing_df = pl.read_csv(\"data/housing-data.csv\")\n\n\n\n\nfour_bed_with_garage = housing_df.filter(pl.col(\"bedrooms\") == 4).filter(pl.col(\"garage\") == True)\nfour_bed_with_garage.head(3)\n\n\n\n\n\nshape: (3, 8)\n\n\n\n\n\n\n\nprice\n\n\nbedrooms\n\n\nbathrooms\n\n\ncouncil_tax\n\n\nfreehold\n\n\ngarage\n\n\nparking\n\n\n\n\ni64\n\n\ni64\n\n\ni64\n\n\ni64\n\n\nstr\n\n\nbool\n\n\nbool\n\n\nbool\n\n\n\n\n\n\n6\n\n\n220000\n\n\n4\n\n\n2\n\n\n\"C\"\n\n\nfalse\n\n\ntrue\n\n\nfalse\n\n\n\n\n11\n\n\n395000\n\n\n4\n\n\n4\n\n\n\"F\"\n\n\ntrue\n\n\ntrue\n\n\nfalse\n\n\n\n\n19\n\n\n334000\n\n\n4\n\n\n4\n\n\n\"F\"\n\n\ntrue\n\n\ntrue\n\n\ntrue\n\n\n\n\n\n\n\n\n\n\n\naverage_price_per_no_of_bedrooms =  housing_df.select(\"bedrooms\", \"price\").groupby(\"bedrooms\").mean().sort(\"bedrooms\")\naverage_price_per_no_of_bedrooms\n\n\n\n\n\nshape: (8, 2)\n\n\n\n\nbedrooms\n\n\nprice\n\n\n\n\ni64\n\n\nf64\n\n\n\n\n\n\n1\n\n\n149998.814442\n\n\n\n\n2\n\n\n199846.936087\n\n\n\n\n3\n\n\n314227.749105\n\n\n\n\n4\n\n\n380126.039318\n\n\n\n\n5\n\n\n465421.410331\n\n\n\n\n6\n\n\n698804.466716\n\n\n\n\n7\n\n\n699474.242897\n\n\n\n\n8\n\n\n699945.904285\n\n\n\n\n\n\n\n\n\n\n\npandas_df = average_price_per_no_of_bedrooms.sort(\"bedrooms\").to_pandas().set_index(\"bedrooms\")\n\nprint(type(pandas_df))\ndisplay(pandas_df)\n\n<class 'pandas.core.frame.DataFrame'>\n\n\n\n\n\n\n  \n    \n      \n      price\n    \n    \n      bedrooms\n      \n    \n  \n  \n    \n      1\n      149998.814442\n    \n    \n      2\n      199846.936087\n    \n    \n      3\n      314227.749105\n    \n    \n      4\n      380126.039318\n    \n    \n      5\n      465421.410331\n    \n    \n      6\n      698804.466716\n    \n    \n      7\n      699474.242897\n    \n    \n      8\n      699945.904285\n    \n  \n\n\n\n\n\n\n\n\nhousing_df.with_columns()"
  },
  {
    "objectID": "run-pg-locally.html",
    "href": "run-pg-locally.html",
    "title": "Blog",
    "section": "",
    "text": "In this tutorial, I’ll show how you can run a Postgres (also known as PostgreSQL) database locally in docker and connect to it using psql.\n\n\nFirst we need to run the postgres container:\ndocker run --name postgres -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres -e POSTGRES_USER=postgres \\\n-e POSTGRES_DB=employees -v ${PWD}/postgres-docker:/var/lib/postgresql/data postgres\nLet’s deconstruct this docker command..\nRun a container called postgres using the postgres image:\n\ndocker run --name postgres [OPTIONS] postgres\n\nRun as a detached container, so it runs in the background of your terminal\n\n-d\n\nMap port 5432 on the localhost to 5432 in the container\n\n-p 5432:5432\n\nNext we pass some Postgres specific environment variables to the Postgres container. You will need to use the User and Password to connect.\n\n-e POSTGRES_PASSWORD=postgres\n-e POSTGRES_USER=postgres\n-e POSTGRES_DB=employees\n\nFinally map the container volumne to a local volume:\n\n-v ${PWD}/postgres-docker:/var/lib/postgresql/data postgres\n\n\n\n\nOnce the container is up-and-running, we can connect to the Postgres database using the sqlalchemy Python package. First, you create an engine object using the Postgres login credentials specified above:\n\n# Import database engine from sqlalchemy\n\nfrom sqlalchemy import create_engine \n\ndb=create_engine(\"postgresql://postgres:postgres@localhost:5432/employees\")\n\nWrite an SQL string to: 1. Create a new table called employee_details 2. Populate this table with some data\n\nbootstrap_sql = \"\"\"\n\nCREATE TABLE EMPLOYEE_DETAILS(\n   ID INT PRIMARY KEY     NOT NULL,\n   NAME           TEXT    NOT NULL,\n   AGE            INT     NOT NULL,\n   ADDRESS        CHAR(50),\n   SALARY         REAL,\n   JOIN_DATE      DATE\n);\n\nINSERT INTO EMPLOYEE_DETAILS (ID,NAME,AGE,ADDRESS,SALARY,JOIN_DATE) \nVALUES (1, 'John', 32, 'London', 20000.00,'2001-07-13');\n\nINSERT INTO EMPLOYEE_DETAILS (ID,NAME,AGE,ADDRESS,SALARY,JOIN_DATE)\nVALUES (2, 'David', 25, 'Dublin', 30000.00, '2007-12-13');\n\nINSERT INTO EMPLOYEE_DETAILS (ID,NAME,AGE,ADDRESS,SALARY,JOIN_DATE)\nVALUES (3, 'Sarah', 25, 'Edinburgh', 40000.00, '2007-12-13');\n\"\"\"\n\nPass that SQL string to the database engine object:\n\nwith db.connect() as con:\n    \n    try:\n\n        rs = con.execute(bootstrap_sql)\n    \n    ## TODO: Add proper error handling\n    except:\n        pass\n\n\n\n\nFirst we’ll want to exec into the postgres container:\ndocker exec -it postgres bash\nOnce you’re in the container you can run psql commands to query data:\nroot@12345abcde1:/#\nroot@12345abcde1:/#  psql -U postgres -d employees -c \"select * from employee_details\"\n\n id | name  | age |                      address                       | salary | join_date\n----+-------+-----+----------------------------------------------------+--------+------------\n  1 | John  |  32 | London                                             |  20000 | 2001-07-13\n  2 | David |  25 | Dublin                                             |  30000 | 2007-12-13\n  3 | Sarah |  25 | Edinburgh                                          |  40000 | 2007-12-13\n\nroot@12345abcde1:/#"
  },
  {
    "objectID": "svp.html",
    "href": "svp.html",
    "title": "Blog",
    "section": "",
    "text": "Data analysis on a running race\n\nIn August of 2017 I ran the SVP100. This is a 100KM trail running race starting in Newmarket (Suffolk, UK) and ending in Manning Tree (Essex, UK). In regular distance marathons, its widely believed runners “hit the wall” at around 30KM mark (or 70% through the race). In this notebook, I want to see if the same holds true over a 100KM distance.\nFrom a high level, the code below performs the following:\n\nScrape results data from the race website (using Beautiful Soup)\nRender scraped data into a DataFrame (using Pandas)\nFormat (or wrangle) the data into formats we can work with\nPresent results in time series graph (using Seaborn)\n\n\n\nFirst, let’s import some packages. We’ll use BeautifulSoup for web scraping, pandas for data analysis, and then seaborn for plotting.\n\n# Import the packages we'll use for our analysis\nimport datetime\nimport time\nimport requests\n\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nThen we set the plotting configuration. I really like the “fivethirtyeight” stylesheet which generates plots in the style used by fivethirtyeight.com.\n\n# Use line magic function to enable matplotlib to work interactively with iPython\n\n%matplotlib inline\n%pylab inline\n\n# Set style to fivethirtyeight to create clean and clear looking graphs\n\nplt.style.use('fivethirtyeight')\n\n# Define a dictionary containing default plotting configurations\n\nparams = {'legend.fontsize': 'small',\n          'figure.figsize': (12, 4.5),\n         'axes.labelsize': 'small',\n         'axes.titlesize':'medium',\n         'xtick.labelsize':'small',\n         'ytick.labelsize':'small'}\n\npylab.rcParams.update(params)\n\nPopulating the interactive namespace from numpy and matplotlib\n\n\nNext define some constants that we’ll use in the notebook. First we define a URL which has a table which contains race result data. Then we have some details about the race distance.\n\n# URL details for requests\nBASE_URL = \"http://www.svp100.co.uk/results-\"\nYEAR = 2017\n\n# Constants of race details\nDISTANCE_BETWEEN_CPS = [19.312, 17.702, 16.094, 17.702, 9.657, 13.679, 7.242]\nCPS_IN_KM = cp_miles = [(i * 1.60934) for i in [12, 23, 33, 44, 50, 58.5, 63]]\n\n\n\n\nIn the function below, we request data from the race website and return a DataFrame. For the most part, the DataFrame is unformatted.\n\n#collapse-hide\n\ndef df_builder(base_url=BASE_URL, year=YEAR):\n    \n    \"\"\"\n    This function returns a pandas DataFrame which contain data scraped from the race website.\n    The data is unformatted.\n    \n    Attributes:\n    -----------\n    base_url (str): the url which contains race data in HTML\n    year (int): the year for we would like data\n    \"\"\"\n    \n    # Scrape the data from the race website\n    url = base_url + str(year)\n    r = requests.get(url).text\n    soup = BeautifulSoup(r, 'lxml')\n    \n    # Find tables from the html\n    rows = soup.find_all('tr')[1:]\n    \n    # Collect and format column names for the dataframe\n    column_html = soup.find_all('th')[:]    \n    columns = [i.contents[0].lower().replace(\"/\",\"\").replace(\" \", \"_\") for i in column_html if i.contents[0]]\n\n    # Build a dataframe\n    data = []\n    for line in rows:\n        row = line.find_all('td')\n        row_list = []\n        for counter, value in enumerate(row):\n            row_list.append(row[counter].string)\n        data.append(row_list)\n    \n    # Remove columns we don't need\n    df = pd.DataFrame(data, columns=columns).drop(columns = [\"name\", \"club\", \"mf\", \"bib\", \"total_time\"]).set_index(\"position\")\n            \n    return df\n\nGenerate a DataFrame of race results for the year 2017.\n\nunformatted_df = df_builder(BASE_URL, YEAR)\nunformatted_df.head(2)\n\n\n\n\n\n  \n    \n      \n      start\n      cp1\n      cp2\n      cp3\n      cp4\n      cp5\n      cp6\n      finish\n    \n    \n      position\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      08:30:00\n      09:58:30\n      11:26:51\n      12:58:00\n      14:34:00\n      15:41:00\n      17:04:49\n      17:54:10\n    \n    \n      2\n      08:30:00\n      09:56:03\n      11:22:10\n      13:20:00\n      15:14:00\n      16:31:00\n      18:13:51\n      19:09:10\n    \n  \n\n\n\n\nScanning the data, we notice a few things…\n\nThe “time” related columns are strings representing a time of day. We’ll need to convert these to datetimes so we can calculate the time (in seconds) between each checkpoint.\nThere are some None values which will need to be removed.\n\n\n\n\nIn the function below, we calculate the time (in seconds) it took each runner to run between checkpoints. Before we start, however, we want to remove any rows which have None values.\n\n# Remove any None rows\nunformatted_df = unformatted_df.dropna().copy()\n\nNow onto our function…\n\n#collapse-hide\n\ndef calculate_time_to_checkpoints(df):\n    \n    \"\"\"\n    This function returns a pandas DataFrame which contains time in seconds it took each\n    runner to reach a checkpoint\n    \n    Attributes:\n    -----------\n    df (DataFrame): a dataframe of race results\n    \"\"\"\n    \n    # List out a set of columns which we'll need to convert into datimes \n    dt_cols = [\"start\", \"cp1\", \"cp2\", \"cp3\", \"cp4\", \"cp5\", \"cp6\", \"finish\"]\n    \n    # Convert these columns to datetimes\n    for col in dt_cols:\n    \n        df[col] = df[col].apply(lambda x: pd.to_datetime(\"2017-08-08 \" + str(x)))\n    \n    # Then calculate the time in seconds between each checkpoint\n    for start, end in zip(dt_cols, dt_cols[1:]):\n        \n        df[f\"time_to_{end}\"] = (df[end] - df[start]).apply(lambda x: x.seconds)\n    \n    # Finally lets drop the old 'time of day' columns\n    df = df.drop(columns=dt_cols).rename(columns={\"time_to_finish\": \"time_to_cp7\"}).copy()\n\n    return df\n\n\ndf_time_to_cp = calculate_time_to_checkpoints(unformatted_df)\ndf_time_to_cp.head(3)\n\n\n\n\n\n  \n    \n      \n      time_to_cp1\n      time_to_cp2\n      time_to_cp3\n      time_to_cp4\n      time_to_cp5\n      time_to_cp6\n      time_to_cp7\n    \n    \n      position\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      5310\n      5301\n      5469\n      5760\n      4020\n      5029\n      2961\n    \n    \n      2\n      5163\n      5167\n      7070\n      6840\n      4620\n      6171\n      3319\n    \n    \n      3\n      6284\n      6382\n      6534\n      6720\n      4440\n      5869\n      3426\n    \n  \n\n\n\n\n\n\n\nNext we calculate the minutes per km it took each runner to move between checkpoints.\n\n#collapse-hide\n\ndef calc_mins_per_km(df):\n    \n    \"\"\"\n    This function returns a pandas DataFrame which contains min per km\n    it took each runner to move between checkpoints\n    \n    Attributes:\n    -----------\n    df (DataFrame): a dataframe of race results\n    \"\"\"\n\n    df = df.copy()\n    \n    for cp_time, distance in zip(df.columns, DISTANCE_BETWEEN_CPS):\n\n        df[f\"min_per_km_{cp_time[-3:]}\"] = df[cp_time].apply(lambda x: (x / distance) / 60)\n\n        df.drop(columns=[cp_time], inplace=True)\n        \n    df.columns = CPS_IN_KM\n        \n    return df\n\nThe DataFrame below shows how long it took to move between each checkpoint. In this DataFrame we’ve also replaced the checkpoint numbers with their distances in kilometers.\n\ndf_min_per_km = calc_mins_per_km(df_time_to_cp)\ndf_min_per_km.head(3)\n\n\n\n\n\n  \n    \n      \n      19.31208\n      37.01482\n      53.10822\n      70.81096\n      80.46700\n      94.14639\n      101.38842\n    \n    \n      position\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      4.582643\n      4.990961\n      5.663601\n      5.423116\n      6.937972\n      6.127397\n      6.814416\n    \n    \n      2\n      4.455779\n      4.864799\n      7.321569\n      6.439950\n      7.973491\n      7.518824\n      7.638314\n    \n    \n      3\n      5.423226\n      6.008737\n      6.766497\n      6.326969\n      7.662835\n      7.150864\n      7.884562\n    \n  \n\n\n\n\n\n\n\nThen we calculate averages for three groups:\n\nAll runners\nThe Top 10 only\nMe only\n\n\nall_runners = pd.DataFrame(df_min_per_km.mean(), columns=[\"all_runners\"])\ntop_10_runners = pd.DataFrame(df_min_per_km.head(10).mean(), columns=[\"top_10\"])\nme = pd.DataFrame(df_min_per_km.filter(items=['7'], axis=0).mean(), columns=[\"me\"])\n\n\nplots = all_runners.join(top_10_runners, lsuffix = \"_all\", rsuffix  = \"_t10\").join(me)\nplots.index.rename(\"kilometer\", inplace=True)\nplots\n\n\n\n\n\n  \n    \n      \n      all_runners\n      top_10\n      me\n    \n    \n      kilometer\n      \n      \n      \n    \n  \n  \n    \n      19.31208\n      6.284585\n      5.366094\n      5.670050\n    \n    \n      37.01482\n      7.424970\n      6.037736\n      6.679095\n    \n    \n      53.10822\n      7.954279\n      6.728595\n      7.162089\n    \n    \n      70.81096\n      8.467717\n      6.880578\n      7.287312\n    \n    \n      80.46700\n      11.937517\n      8.594802\n      9.009009\n    \n    \n      94.14639\n      9.395034\n      7.693545\n      7.772254\n    \n    \n      101.38842\n      9.952413\n      7.907806\n      7.764890\n    \n  \n\n\n\n\n\n\n\nLooking at the results below, we can see that all three groups move in a similar pattern, slowing down substantially at 80KM, before picking up the pace for the final 20KM.\n\nplots.plot(y = [\"all_runners\",\"top_10\",\"me\"], figsize=(12,7))\nplt.title('Average pace of runners (minutes per km)')\nplt.ylabel('Minutes per km', fontsize=\"small\")\nplt.xlabel('Kilometers')\nplt.show()"
  }
]