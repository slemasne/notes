[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi There! I am Stephen Lemasney. I work in FinTech as a Sales Engineer. In this blog I share some Python and data tutorials."
  },
  {
    "objectID": "build-a-test-csv-dataset.html",
    "href": "build-a-test-csv-dataset.html",
    "title": "Blog",
    "section": "",
    "text": "In this tutorial, we’ll build a test CSV which we’ll load onto AWS S3. The dataset will be a simple housing dataset showing attributes such as price, number of bedrooms etc. The data is entirely fabricated.\n\n\n\nimport pandas as pd\nimport tomli\nimport pprint\n\n# We'll use random type functions to generate the random house price data\nfrom random import choice, randint\nfrom numpy.random import normal\n\n\n\n\nWe’ll store our config in a TOML file. The config will contain basic instructions for creating the test data. For example, we specify a min and a max when generating a random int for “number of bedrooms”.\nContents of config/housing.toml\n# Config for creating a housing dataset\n\n[price]\ntype = \"dist\"\nmean = 500000\nsd = 200000\n\n[area]\ntype = \"min_max\"\nmax = 300\nmin = 50\n\n[bathrooms]\ntype = \"min_max\"\nmax = 5\nmin = 1\n\n[bedrooms]\ntype = \"min_max\"\nmax = 5\nmin = 1\n\n[garage]\ntype = \"bool\"\n\n[parking]\ntype = \"bool\"\n\n[council_tax_band]\ntype = \"list\"\nlist = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\nwith open('config/housing.toml', 'rb') as file_obj:\n    housing_config = tomli.load(file_obj)\n    \npprint.pprint(housing_config)\n\n{'area': {'max': 300, 'min': 50, 'type': 'min_max'},\n 'bathrooms': {'max': 5, 'min': 1, 'type': 'min_max'},\n 'bedrooms': {'max': 5, 'min': 1, 'type': 'min_max'},\n 'council_tax_band': {'list': ['A', 'B', 'C', 'D', 'E'], 'type': 'list'},\n 'garage': {'type': 'bool'},\n 'parking': {'type': 'bool'},\n 'price': {'mean': 500000, 'sd': 200000, 'type': 'dist'}}\n\n\n\n\n\nNext we build our random data using the TOML file. We have two functions below:\n\nThe first returns a random data point based on the config passed in\nThe second builds a DataFrame using these data points\n\n\ndef build_random_housing_column_value(attribute):\n        \n    if attribute[\"type\"] == \"min_max\":\n        \n        return randint(attribute[\"min\"], attribute[\"max\"] )\n    \n    elif attribute.get(\"type\") == \"bool\":\n        \n        return choice([True, False])\n    \n    elif attribute.get(\"type\") == \"dist\":\n        \n        sample_price = normal(loc=attribute[\"mean\"], scale=attribute[\"sd\"], size=1)[0]\n        \n        return int(round(sample_price, -3))\n    \n    elif attribute.get(\"type\") == \"list\":\n        \n        return choice(attribute[\"list\"])\n    \n\n\ndef generate_housing_data_df(sample_size=1000000):\n\n    df_cols = {}\n\n    for col in housing_config:\n\n        column_config = housing_config[col]\n\n        df_cols[col] = [build_random_housing_column_value(column_config) for i in range(0, sample_size)]\n        \n    return  pd.DataFrame(df_cols)\n\n\ndf = generate_housing_data_df()\ndf.head(3)\n\n\n\n\n\n  \n    \n      \n      price\n      area\n      bathrooms\n      bedrooms\n      garage\n      parking\n      council_tax_band\n    \n  \n  \n    \n      0\n      695000\n      202\n      2\n      4\n      False\n      True\n      B\n    \n    \n      1\n      574000\n      218\n      5\n      4\n      True\n      True\n      E\n    \n    \n      2\n      408000\n      228\n      4\n      1\n      True\n      True\n      D\n    \n  \n\n\n\n\n\n# Check the statistical dispersion of the sample data\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      price\n      area\n      bathrooms\n      bedrooms\n    \n  \n  \n    \n      count\n      1.000000e+06\n      1000000.000000\n      1000000.000000\n      1000000.000000\n    \n    \n      mean\n      4.997278e+05\n      175.034369\n      3.000174\n      2.999759\n    \n    \n      std\n      2.000181e+05\n      72.434211\n      1.413240\n      1.412847\n    \n    \n      min\n      -4.700000e+05\n      50.000000\n      1.000000\n      1.000000\n    \n    \n      25%\n      3.650000e+05\n      112.000000\n      2.000000\n      2.000000\n    \n    \n      50%\n      5.000000e+05\n      175.000000\n      3.000000\n      3.000000\n    \n    \n      75%\n      6.350000e+05\n      238.000000\n      4.000000\n      4.000000\n    \n    \n      max\n      1.502000e+06\n      300.000000\n      5.000000\n      5.000000\n    \n  \n\n\n\n\n\n\n\nFinally, we save the CSV file to S3. Fortunately, the Pandas API does all of the “heavy lifting” to connect to AWS.\n\nS3_BUCKET_LOCATION = \"<BUCKET_LOCATION>\"\n\ndf.to_csv(S3_BUCKET_LOCATION)"
  },
  {
    "objectID": "flask-app-with-docker.html",
    "href": "flask-app-with-docker.html",
    "title": "Blog",
    "section": "",
    "text": "In this tutorial, we show how you can build and deploy a really simple Python app using Docker. To demonstrate, we’ll create an API to return details of the TFL train lines in London. We’ll use Flask to build the application.\n\n\nFirst, create a python flask app which returns details about the TFL trains. This application serves a very basic API with details on London’s TFL trains.\n\n#collapse-hide\n\nfrom flask import Flask, jsonify, request\nimport sys\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\napp = Flask(__name__)\n\ntasks = [\n    {\n        'id': 1,\n        'service': 'tube',\n        'line': 'northern',\n        'colour': 'black'\n    },\n    {\n        'id': 2,\n        'service': 'tube',\n        'line': 'circle',\n        'colour': 'red'\n    }\n]\n\ndef shutdown_server():\n    func = request.environ.get('werkzeug.server.shutdown')\n    if func is None:\n        raise RuntimeError('Not running with the Werkzeug Server')\n    func()\n\n@app.route('/trains', methods=['GET'])\ndef get_tasks():\n    return jsonify({'tasks': tasks})\n\n@app.route('/')\ndef hello_world():\n    return 'Welcome to trains API'\n\n@app.route('/exit')\ndef exit():\n    message = logging.info(\"Stopping application\")\n    shutdown_server()\n    print(\"The Flask server has been shutdown.\")\n\nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0')\n\n\n\n\n\nimport requests\n\nurl = \"http://192.168.1.74:5000/\"\n\nresponse = requests.get(url)\nresponse_code = response.status_code\nresponse_text = requests.get(url).text\n\ndisplay(response_code)\ndisplay(response_text)\n\n200\n\n\n'Welcome to trains API'\n\n\n\n\n\nNext, we want to package the application into a container using a Dockerfile. The finished Dockerfile will look as follows:\nFROM python:3.7-alpine\n\nCOPY . /app\n\nWORKDIR /app\n\nRUN pip install -r requirements.txt\n\nEXPOSE 5000\n\nCMD [\"python\", \"app.py\"]\n\n\n\nLet’s break down each line in turn to describe what is happening…\nIn the first line we declare a parent image which is the image our own image is based on. Each subsequent declaration in the Dockerfile modifies this image. In our example, we use a version of Alpine Linux as our base image. Alpine Linux is a lightweight Linux distribution which makes it ideal for our container.\nFROM python:3.7-alpine\nNext we copy all of the files from the current host directory into the container’s app directory.\nCOPY . /app\nWe change our working directory to the app directory. And then we tell Docker to install the Python packages needed for the app.py\nWORKDIR /app\n\nRUN pip install -r requirements.txt\nNext we tell the container to listen on a specific network port at runtime. The default is TCP but you can also specify UDP. Note that the EXPOSE instruction does not actually publish the port. This instruction is there for documentation purposes. To actually publish the port, you need to use the -p flag on the docker run command.\nEXPOSE 5000\nFinally, we run the app:\nCMD [\"python\", \"app.py\"]\n\n\n\nYou might want to run the Dockerfile on your own machine to verify that it is working correctly. To do that, we docker build and then docker run:\nYou can build the Dockerfile as follows. The -t or tag allows you to tag the image with a name.\ndocker build -t slemasne/trains .\nThen run the image to create a container which runs our application. The ‘-p’ flag maps port 5000 on localhost in the host to port 5000 in the docker container.\ndocker run -p 5000:5000 slemasne/trains\nYou can also run the command with a ‘-d’ detached flag to run the container in the background:\ndocker run -d -p 5000:5000 slemasne/trains\nThe application can be accessed on our localhost:\nhttp://localhost:5000/trains"
  },
  {
    "objectID": "flask-app-with-k8s.html",
    "href": "flask-app-with-k8s.html",
    "title": "Blog",
    "section": "",
    "text": "In Part 1 of this tutorial, we deployed a simple Flask app using docker. In this part, we’ll deploy that same app using Kubernetes.\n\n\nFor the Kubernetes deployment, we’ll need to create a deployment for managing pods/containers.\nWe create the deployment using a YAML file.\ntrains-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trains\nspec:\n  selector:\n    matchLabels:\n      app: trains\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: trains\n    spec:\n      containers:\n      - name: trains\n        image: slemasne/trains:latest\n        resources:\n          limits:\n            memory: \"100Mi\"\n          requests:\n            memory: \"50Mi\"\n        ports:\n        - containerPort: 5000\n\n\n\nWe can see that two pods are running using the following command:\n$ kubectl get pods\n\nNAME                      READY   STATUS    RESTARTS   AGE\ntrains-758d4f498c-kbs9k   1/1     Running   0          6m19s\ntrains-758d4f498c-kfbpq   1/1     Running   1          6m19s\n\n\n\nNext we run a port-forward command so we can connect to the cluster from our local machine.\nkubectl port-forward deployment/trains 5000:5000\nWe check the pod is online by running a curl command.\n$ curl -I http://127.0.0.1:5000/\nThis command returns a status code of 200:\nHTTP/1.0 200 OK\nContent-Type: text/html; charset=utf-8\nContent-Length: 21\nServer: Werkzeug/1.0.1 Python/3.7.9\nDate: Wed, 14 Sep 2022 18:01:56 GMT"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stephen Lemasney",
    "section": "",
    "text": "A blog of mostly Python and data tutorials.\nBuild a test CSV dataset\nRun a Postres Database locally\nData analysis on long distance running\nDeploying a Python app - Part 1 (Docker)\nDeploying a Python app - Part 2 (Kubernetes)"
  },
  {
    "objectID": "run-pg-locally.html",
    "href": "run-pg-locally.html",
    "title": "Blog",
    "section": "",
    "text": "In this tutorial, I’ll show how you can run a Postgres (also known as PostgreSQL) database locally in docker and connect to it using psql.\n\n\nFirst we need to run the postgres container:\ndocker run --name postgres -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres -e POSTGRES_USER=postgres \\\n-e POSTGRES_DB=employees -v ${PWD}/postgres-docker:/var/lib/postgresql/data postgres\nLet’s deconstruct this docker command..\nRun a container called postgres using the postgres image:\n\ndocker run --name postgres [OPTIONS] postgres\n\nRun as a detached container, so it runs in the background of your terminal\n\n-d\n\nMap port 5432 on the localhost to 5432 in the container\n\n-p 5432:5432\n\nNext we pass some Postgres specific environment variables to the Postgres container. You will need to use the User and Password to connect.\n\n-e POSTGRES_PASSWORD=postgres\n-e POSTGRES_USER=postgres\n-e POSTGRES_DB=employees\n\nFinally map the container volumne to a local volume:\n\n-v ${PWD}/postgres-docker:/var/lib/postgresql/data postgres\n\n\n\n\nOnce the container is up-and-running, we can connect to the Postgres database using the sqlalchemy Python package. First, you create an engine object using the Postgres login credentials specified above:\n\n# Import database engine from sqlalchemy\n\nfrom sqlalchemy import create_engine \n\ndb=create_engine(\"postgresql://postgres:postgres@localhost:5432/employees\")\n\nWrite an SQL string to: 1. Create a new table called employee_details 2. Populate this table with some data\n\nbootstrap_sql = \"\"\"\n\nCREATE TABLE EMPLOYEE_DETAILS(\n   ID INT PRIMARY KEY     NOT NULL,\n   NAME           TEXT    NOT NULL,\n   AGE            INT     NOT NULL,\n   ADDRESS        CHAR(50),\n   SALARY         REAL,\n   JOIN_DATE      DATE\n);\n\nINSERT INTO EMPLOYEE_DETAILS (ID,NAME,AGE,ADDRESS,SALARY,JOIN_DATE) \nVALUES (1, 'John', 32, 'London', 20000.00,'2001-07-13');\n\nINSERT INTO EMPLOYEE_DETAILS (ID,NAME,AGE,ADDRESS,SALARY,JOIN_DATE)\nVALUES (2, 'David', 25, 'Dublin', 30000.00, '2007-12-13');\n\nINSERT INTO EMPLOYEE_DETAILS (ID,NAME,AGE,ADDRESS,SALARY,JOIN_DATE)\nVALUES (3, 'Sarah', 25, 'Edinburgh', 40000.00, '2007-12-13');\n\"\"\"\n\nPass that SQL string to the database engine object:\n\nwith db.connect() as con:\n    \n    try:\n\n        rs = con.execute(bootstrap_sql)\n    \n    ## TODO: Add proper error handling\n    except:\n        pass\n\n\n\n\nFirst we’ll want to exec into the postgres container:\ndocker exec -it postgres bash\nOnce you’re in the container you can run psql commands to query data:\nroot@12345abcde1:/#\nroot@12345abcde1:/#  psql -U postgres -d employees -c \"select * from employee_details\"\n\n id | name  | age |                      address                       | salary | join_date\n----+-------+-----+----------------------------------------------------+--------+------------\n  1 | John  |  32 | London                                             |  20000 | 2001-07-13\n  2 | David |  25 | Dublin                                             |  30000 | 2007-12-13\n  3 | Sarah |  25 | Edinburgh                                          |  40000 | 2007-12-13\n\nroot@12345abcde1:/#"
  },
  {
    "objectID": "svp.html",
    "href": "svp.html",
    "title": "Blog",
    "section": "",
    "text": "Data analysis on a running race\n\nIn August of 2017 I ran the SVP100. This is a 100KM trail running race starting in Newmarket (Suffolk, UK) and ending in Manning Tree (Essex, UK). In regular distance marathons, its widely believed runners “hit the wall” at around 30KM mark (or 70% through the race). In this notebook, I want to see if the same holds true over a 100KM distance.\nFrom a high level, the code below performs the following:\n\nScrape results data from the race website (using Beautiful Soup)\nRender scraped data into a DataFrame (using Pandas)\nFormat (or wrangle) the data into formats we can work with\nPresent results in time series graph (using Seaborn)\n\n\n\nFirst, let’s import some packages. We’ll use BeautifulSoup for web scraping, pandas for data analysis, and then seaborn for plotting.\n\n# Import the packages we'll use for our analysis\nimport datetime\nimport time\nimport requests\n\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nThen we set the plotting configuration. I really like the “fivethirtyeight” stylesheet which generates plots in the style used by fivethirtyeight.com.\n\n# Use line magic function to enable matplotlib to work interactively with iPython\n\n%matplotlib inline\n%pylab inline\n\n# Set style to fivethirtyeight to create clean and clear looking graphs\n\nplt.style.use('fivethirtyeight')\n\n# Define a dictionary containing default plotting configurations\n\nparams = {'legend.fontsize': 'small',\n          'figure.figsize': (12, 4.5),\n         'axes.labelsize': 'small',\n         'axes.titlesize':'medium',\n         'xtick.labelsize':'small',\n         'ytick.labelsize':'small'}\n\npylab.rcParams.update(params)\n\nPopulating the interactive namespace from numpy and matplotlib\n\n\nNext define some constants that we’ll use in the notebook. First we define a URL which has a table which contains race result data. Then we have some details about the race distance.\n\n# URL details for requests\nBASE_URL = \"http://www.svp100.co.uk/results-\"\nYEAR = 2017\n\n# Constants of race details\nDISTANCE_BETWEEN_CPS = [19.312, 17.702, 16.094, 17.702, 9.657, 13.679, 7.242]\nCPS_IN_KM = cp_miles = [(i * 1.60934) for i in [12, 23, 33, 44, 50, 58.5, 63]]\n\n\n\n\nIn the function below, we request data from the race website and return a DataFrame. For the most part, the DataFrame is unformatted.\n\n#collapse-hide\n\ndef df_builder(base_url=BASE_URL, year=YEAR):\n    \n    \"\"\"\n    This function returns a pandas DataFrame which contain data scraped from the race website.\n    The data is unformatted.\n    \n    Attributes:\n    -----------\n    base_url (str): the url which contains race data in HTML\n    year (int): the year for we would like data\n    \"\"\"\n    \n    # Scrape the data from the race website\n    url = base_url + str(year)\n    r = requests.get(url).text\n    soup = BeautifulSoup(r, 'lxml')\n    \n    # Find tables from the html\n    rows = soup.find_all('tr')[1:]\n    \n    # Collect and format column names for the dataframe\n    column_html = soup.find_all('th')[:]    \n    columns = [i.contents[0].lower().replace(\"/\",\"\").replace(\" \", \"_\") for i in column_html if i.contents[0]]\n\n    # Build a dataframe\n    data = []\n    for line in rows:\n        row = line.find_all('td')\n        row_list = []\n        for counter, value in enumerate(row):\n            row_list.append(row[counter].string)\n        data.append(row_list)\n    \n    # Remove columns we don't need\n    df = pd.DataFrame(data, columns=columns).drop(columns = [\"name\", \"club\", \"mf\", \"bib\", \"total_time\"]).set_index(\"position\")\n            \n    return df\n\nGenerate a DataFrame of race results for the year 2017.\n\nunformatted_df = df_builder(BASE_URL, YEAR)\nunformatted_df.head(2)\n\n\n\n\n\n  \n    \n      \n      start\n      cp1\n      cp2\n      cp3\n      cp4\n      cp5\n      cp6\n      finish\n    \n    \n      position\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      08:30:00\n      09:58:30\n      11:26:51\n      12:58:00\n      14:34:00\n      15:41:00\n      17:04:49\n      17:54:10\n    \n    \n      2\n      08:30:00\n      09:56:03\n      11:22:10\n      13:20:00\n      15:14:00\n      16:31:00\n      18:13:51\n      19:09:10\n    \n  \n\n\n\n\nScanning the data, we notice a few things…\n\nThe “time” related columns are strings representing a time of day. We’ll need to convert these to datetimes so we can calculate the time (in seconds) between each checkpoint.\nThere are some None values which will need to be removed.\n\n\n\n\nIn the function below, we calculate the time (in seconds) it took each runner to run between checkpoints. Before we start, however, we want to remove any rows which have None values.\n\n# Remove any None rows\nunformatted_df = unformatted_df.dropna().copy()\n\nNow onto our function…\n\n#collapse-hide\n\ndef calculate_time_to_checkpoints(df):\n    \n    \"\"\"\n    This function returns a pandas DataFrame which contains time in seconds it took each\n    runner to reach a checkpoint\n    \n    Attributes:\n    -----------\n    df (DataFrame): a dataframe of race results\n    \"\"\"\n    \n    # List out a set of columns which we'll need to convert into datimes \n    dt_cols = [\"start\", \"cp1\", \"cp2\", \"cp3\", \"cp4\", \"cp5\", \"cp6\", \"finish\"]\n    \n    # Convert these columns to datetimes\n    for col in dt_cols:\n    \n        df[col] = df[col].apply(lambda x: pd.to_datetime(\"2017-08-08 \" + str(x)))\n    \n    # Then calculate the time in seconds between each checkpoint\n    for start, end in zip(dt_cols, dt_cols[1:]):\n        \n        df[f\"time_to_{end}\"] = (df[end] - df[start]).apply(lambda x: x.seconds)\n    \n    # Finally lets drop the old 'time of day' columns\n    df = df.drop(columns=dt_cols).rename(columns={\"time_to_finish\": \"time_to_cp7\"}).copy()\n\n    return df\n\n\ndf_time_to_cp = calculate_time_to_checkpoints(unformatted_df)\ndf_time_to_cp.head(3)\n\n\n\n\n\n  \n    \n      \n      time_to_cp1\n      time_to_cp2\n      time_to_cp3\n      time_to_cp4\n      time_to_cp5\n      time_to_cp6\n      time_to_cp7\n    \n    \n      position\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      5310\n      5301\n      5469\n      5760\n      4020\n      5029\n      2961\n    \n    \n      2\n      5163\n      5167\n      7070\n      6840\n      4620\n      6171\n      3319\n    \n    \n      3\n      6284\n      6382\n      6534\n      6720\n      4440\n      5869\n      3426\n    \n  \n\n\n\n\n\n\n\nNext we calculate the minutes per km it took each runner to move between checkpoints.\n\n#collapse-hide\n\ndef calc_mins_per_km(df):\n    \n    \"\"\"\n    This function returns a pandas DataFrame which contains min per km\n    it took each runner to move between checkpoints\n    \n    Attributes:\n    -----------\n    df (DataFrame): a dataframe of race results\n    \"\"\"\n\n    df = df.copy()\n    \n    for cp_time, distance in zip(df.columns, DISTANCE_BETWEEN_CPS):\n\n        df[f\"min_per_km_{cp_time[-3:]}\"] = df[cp_time].apply(lambda x: (x / distance) / 60)\n\n        df.drop(columns=[cp_time], inplace=True)\n        \n    df.columns = CPS_IN_KM\n        \n    return df\n\nThe DataFrame below shows how long it took to move between each checkpoint. In this DataFrame we’ve also replaced the checkpoint numbers with their distances in kilometers.\n\ndf_min_per_km = calc_mins_per_km(df_time_to_cp)\ndf_min_per_km.head(3)\n\n\n\n\n\n  \n    \n      \n      19.31208\n      37.01482\n      53.10822\n      70.81096\n      80.46700\n      94.14639\n      101.38842\n    \n    \n      position\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      4.582643\n      4.990961\n      5.663601\n      5.423116\n      6.937972\n      6.127397\n      6.814416\n    \n    \n      2\n      4.455779\n      4.864799\n      7.321569\n      6.439950\n      7.973491\n      7.518824\n      7.638314\n    \n    \n      3\n      5.423226\n      6.008737\n      6.766497\n      6.326969\n      7.662835\n      7.150864\n      7.884562\n    \n  \n\n\n\n\n\n\n\nThen we calculate averages for three groups:\n\nAll runners\nThe Top 10 only\nMe only\n\n\nall_runners = pd.DataFrame(df_min_per_km.mean(), columns=[\"all_runners\"])\ntop_10_runners = pd.DataFrame(df_min_per_km.head(10).mean(), columns=[\"top_10\"])\nme = pd.DataFrame(df_min_per_km.filter(items=['7'], axis=0).mean(), columns=[\"me\"])\n\n\nplots = all_runners.join(top_10_runners, lsuffix = \"_all\", rsuffix  = \"_t10\").join(me)\nplots.index.rename(\"kilometer\", inplace=True)\nplots\n\n\n\n\n\n  \n    \n      \n      all_runners\n      top_10\n      me\n    \n    \n      kilometer\n      \n      \n      \n    \n  \n  \n    \n      19.31208\n      6.284585\n      5.366094\n      5.670050\n    \n    \n      37.01482\n      7.424970\n      6.037736\n      6.679095\n    \n    \n      53.10822\n      7.954279\n      6.728595\n      7.162089\n    \n    \n      70.81096\n      8.467717\n      6.880578\n      7.287312\n    \n    \n      80.46700\n      11.937517\n      8.594802\n      9.009009\n    \n    \n      94.14639\n      9.395034\n      7.693545\n      7.772254\n    \n    \n      101.38842\n      9.952413\n      7.907806\n      7.764890\n    \n  \n\n\n\n\n\n\n\nLooking at the results below, we can see that all three groups move in a similar pattern, slowing down substantially at 80KM, before picking up the pace for the final 20KM.\n\nplots.plot(y = [\"all_runners\",\"top_10\",\"me\"], figsize=(12,7))\nplt.title('Average pace of runners (minutes per km)')\nplt.ylabel('Minutes per km', fontsize=\"small\")\nplt.xlabel('Kilometers')\nplt.show()"
  }
]